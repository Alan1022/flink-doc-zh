

# Checkpoints

## Overview

Checkpoints make state in Flink fault tolerant by allowing state and the corresponding stream positions to be recovered, thereby giving the application the same semantics as a failure-free execution.

See [Checkpointing](//ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/checkpointing.html) for how to enable and configure checkpoints for your program.

## Retained Checkpoints

Checkpoints are by default not retained and are only used to resume a job from failures. They are deleted when a program is cancelled. You can, however, configure periodic checkpoints to be retained. Depending on the configuration these _retained_ checkpoints are _not_ automatically cleaned up when the job fails or is canceled. This way, you will have a checkpoint around to resume from if your job fails.

<figure class="highlight">

```
CheckpointConfig config = env.getCheckpointConfig();
config.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
```

</figure>

The `ExternalizedCheckpointCleanup` mode configures what happens with checkpoints when you cancel the job:

*   **`ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION`**: Retain the checkpoint when the job is cancelled. Note that you have to manually clean up the checkpoint state after cancellation in this case.

*   **`ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION`**: Delete the checkpoint when the job is cancelled. The checkpoint state will only be available if the job fails.

### Directory Structure

Similarly to [savepoints](savepoints.html), a checkpoint consists of a meta data file and, depending on the state backend, some additional data files. The meta data file and data files are stored in the directory that is configured via `state.checkpoints.dir` in the configuration files, and also can be specified for per job in the code.

#### Configure globally via configuration files

<figure class="highlight">

```
state.checkpoints.dir: hdfs:///checkpoints/
```

</figure>

#### Configure for per job when constructing the state backend

<figure class="highlight">

```
env.setStateBackend(new RocksDBStateBackend("hdfs:///checkpoints-data/");
```

</figure>

### Difference to Savepoints

Checkpoints have a few differences from [savepoints](savepoints.html). They

*   use a state backend specific (low-level) data format, may be incremental.
*   do not support Flink specific features like rescaling.

### Resuming from a retained checkpoint

A job may be resumed from a checkpoint just as from a savepoint by using the checkpointâ€™s meta data file instead (see the [savepoint restore guide](../cli.html#restore-a-savepoint)). Note that if the meta data file is not self-contained, the jobmanager needs to have access to the data files it refers to (see [Directory Structure](#directory-structure) above).

<figure class="highlight">

```
$ bin/flink run -s :checkpointMetaDataPath [:runArgs]
```

</figure>

